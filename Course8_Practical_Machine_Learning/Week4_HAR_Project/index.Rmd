---
title: "Coursera Machine Learning Project"
author: "Janamejaya Chowdhary"
date: "June 25, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

Barbell lifts were performed by six participants. Each participant attempted the barbell lift in five different ways: according to specification, i.e., correct method (Class A), throwing the elbow to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D), and throwing the hips to the front (Class E). Classes B to E are incorrect ways of performing the task. Data is generated by accelerometers on the belt, forearm, arm, and dumbbell of each participant as they perform the lifts. Each measurement was made 10 times. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har[1] (see the section on the Weight Lifting Exercise Dataset). 

### Objective
The goal of your project is to predict the manner in which the participants performed the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

### Data Ingestion (aka Acquiring and loading data)
The data is provided as part of the project from the following URLs.
```{r assign_url_to_data}
# Assign the URL from which the training and testing data will be downloaded
urltrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urltest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```

First, this data is downloaded to the local directory called data and loaded into data frames
```{r download_data}
# If not downloaded, download data from predefined urls urltrain and urltest
if(!file.exists("./data"))
{ 
 dir.create("./data")
 download.file(urltrain,destfile="./data/training.csv", method="curl")
 download.file(urltest,destfile="./data/testing.csv", method="curl")
}

# Load in the data.
# Previous experimentation shows several occurances of NA, "", and "#DIV/0!"
# These three strings are set as possible NA values
training <- read.csv("./data/training.csv", na.strings=c("NA", "", "#DIV/0!"), stringsAsFactors=FALSE)
testing <- read.csv("./data/testing.csv", na.strings=c("NA", "", "#DIV/0!"), stringsAsFactors=FALSE)
```

### Data Mastication (aka Preprocessing)
Lets take a look at the size of the training and testing data
```{r check_dims}
dim(training); dim(testing)
```
The number of rows in the training dataset, `r dim(training)[1]`, is much larger than the testing dataset which has `r dim(testing)[1]` rows. 

#### Common and distinct variables in Training and Testing data
Check how many columns in the training and testing datasets have identical names.
```{r check_number_common_column_names}
sum(colnames(training)==colnames(testing))
```
This number differs from the number of columns, `r length(colnames(training))` in the training and testing datasets. Lets find out which one column is unique to the training and testing datasets
```{r check_identical_columns}
setdiff(colnames(training), colnames(testing))
setdiff(colnames(testing), colnames(training))
```

Set classe to be a factor variable. Since this is the target variable, lets look at the distribution of values for each unique classe value.
```{r set_classe_to_factor}
training$classe <- as.factor(training$classe)
summary(training$classe)
```
Clearly no unique classe is underrepresented and additional sampling is not required.

#### Remove non-accelerometer data columns from Training data
In the training dataset, the first 7 variables `r colnames(training)[1:7]` do not appear to contain any accelerometer related information and they should be ignored. Implicit in this decision is the assumption that the name of the user and the time stamps do not contain any information that could be used through a model for making predictions on the test dataset.
```{r drop_columns}
training <- training[,-c(1:7)]
testing <- testing[,-c(1:7)]
```

#### Remove columns with majority of missing values in Training data
Now find the fraction of NA values in each column and remove those columns for which more than 30% of the data is missing.
```{r find_missing_in_training}
colselect <- (colSums(is.na(training))/nrow(training))<0.3
training <- training[colselect]
```

#### Remove columns showing near zero variation in Training data
Next, load the caret library and identify the columns that have near zero variation. Remove them.
```{r remove_zero, warning=FALSE, message=FALSE}
library(caret)
# Find the nearzero variance data columns in the training dataframe
tmp1 <- nearZeroVar(training)
# Remove all nearzero variance columns
if(length(tmp1)>0) { training <- training[,-tmp1] }
```

#### Remove columns of correlated variables in Training data
Next remove all variables from the training dataset which have absolute value of the correlation coefficient larger than a cutoff of 0.95
```{r remove_large_correlations}
corrmat <- cor(training[-ncol(training)])
removecols <- findCorrelation(corrmat, cutoff=0.95)
training <- training[-removecols]
```

#### Prune the testing dataset
Next prune the testing dataset by retaining only those columns which are common with the training dataset
```{r prune_testing_dataset}
# Retain data for all remaining variables in training,other than classe, in the
# testing dataset
testing <- testing[,colnames(training[,-ncol(training)])]
dim(training); dim(testing)
```

Now look for the number of columns with missing values in the testing data
```{r check_missing_test_data}
sum(colSums(is.na(training))>0); sum(colSums(is.na(testing))>0)
```

### Data Digestion (aka Modeling the data)

#### Model choice
Several models such as Random Forests (RF), Gradient Boosted Model, Linear Discriminant Analysis, Support Vector Machines, and regression using LASSO were applied for the classification task (comparative analysis not presented). The most successful model was found to be Random Forests and will be utilized in the following.

#### Set up cluster for parallel processing
```{r setup_parallel, warning=FALSE, message=FALSE}
library(parallel)
library(doParallel)
ncores <- detectCores()-1
cluster <- makeCluster(ncores)
registerDoParallel(cluster)
```

#### Test-Validation data Split
The training data is split into 70% test and 30% validation sets.
```{r split_training_data}
inTrain   <- createDataPartition(training$classe, p=0.75, list=FALSE, times=1)
trainData <- training[inTrain, ]
validData  <- training[-inTrain, ]
```

#### Set controls for training the model
Ten fold cross-validation will be used for model training.
```{r use_cv_for_training_model}
set.seed(201706)
nfolds=10
fitControl <- trainControl(method="cv", number = nfolds, allowParallel = TRUE, seeds=NA)
```

#### Building and Testing the Random Forest model
Here, trainData is used to train and test the Random Forest model.
```{r fit_rf, warning=FALSE, message=FALSE}
rfFit <- train(classe~., data=trainData, method="rf", trControl=fitControl)
```
Make predictions with the rfFit model on the training data
```{r predict_w_rf_train}
pred_rf <- predict(rfFit, trainData)
confusionMatrix(pred_rf, trainData$classe)$overall[1]
```
Make predictions with the rfFit model on the validation data
```{r predict_w_rf_test}
pred_rf <- predict(rfFit, validData)
confusionMatrix(pred_rf, validData$classe)$overall[1]
```
The model appears to be very sucessful with default setting and no parameter turning is performed. The out of sample error can be calculated as
```{r out_of_sample_error}
error <- 1.0 - confusionMatrix(pred_rf, validData$classe)$overall[1]
out_of_sample_error <- data.frame(out_of_sample_error=c(error))
out_of_sample_error
```

#### Predicting with the model
Now make predictions for the testing data
```{r predict_test}
pred_rf <- predict(rfFit, testing)
pred_rf
```

#### Shutdown the cluster
```{r shutdown_parallel_processing}
stopCluster(cluster)
registerDoSEQ()
```

[1]:Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.